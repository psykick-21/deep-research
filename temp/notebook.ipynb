{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from typing import TypedDict, List, Annotated, Literal, Union\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import operator\n",
    "\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import uuid\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(\n",
    "        provider: Literal[\"openai\", \"anthropic\", \"google\", \"ollama\"],\n",
    "        model: str,\n",
    "        temperature: float = 0.5,\n",
    "):\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(model=model, temperature=temperature)\n",
    "    elif provider == \"anthropic\":\n",
    "        return ChatAnthropic(model=model, temperature=temperature)\n",
    "    elif provider == \"google\":\n",
    "        return ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(model=model, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm(\n",
    "    provider=\"ollama\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    section_name: str = Field(..., description=\"The name of this section of the report without its number\")\n",
    "    sub_sections: List[str] = Field(..., description=\"Comprehensive descriptions of sub-sections, each combining the sub-section title and its bullet points into a fluid, natural-language description\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(..., description=\"A list of sections\")\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str = Field(..., description=\"A search query\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[Query] = Field(..., description=\"A list of search queries\")\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    query: Query = Field(..., description=\"The search query that was used to retrieve the raw content\")\n",
    "    raw_content: list[str] = Field(..., description=\"The raw content retrieved from the search\")\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    feedback: Union[str, bool] = Field(..., description=\"Feedback on the report structure. If the content is good for the section, return True (boolean), otherwise return a string of feedback on what is missing or incorrect.\")\n",
    "\n",
    "class SectionOutput(BaseModel):\n",
    "    final_section_content: List[str] = Field(..., description=\"The final section content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    outline: str\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    report_structure: str\n",
    "    sections: List[Section]\n",
    "    final_section_content: Annotated[List[str], operator.add] = []\n",
    "    final_report_content: str\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    section: Section\n",
    "    knowledge: str\n",
    "    reflection_feedback: Feedback = Feedback(feedback=\"\")\n",
    "    generated_queries: List[Query] = []\n",
    "    searched_queries: Annotated[List[Query], operator.add] = []\n",
    "    search_results: Annotated[List[SearchResult], operator.add] = []\n",
    "    accumulated_content: str = \"\"\n",
    "    reflection_count: int = 1\n",
    "    final_section_content: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_STRUCTURE_PLANNER_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are an expert research assistant specialized in creating structured research frameworks. Your primary task is to generate a detailed, appropriate report structure based on a user's research topic and brief outline.\n",
    "\n",
    "## Process to Follow:\n",
    "\n",
    "1. UNDERSTAND THE REQUEST:\n",
    "   - Carefully analyze the topic and outline provided by the user\n",
    "   - Identify the type of research needed (exploratory, comparative, analytical, etc.)\n",
    "   - Recognize the domain/field of the research\n",
    "\n",
    "2. ASK CLARIFYING QUESTIONS:\n",
    "   - If the user's request lacks sufficient detail, ask 2-3 focused questions to better understand:\n",
    "     * Their background and expertise level\n",
    "     * Their specific goals for the research\n",
    "     * Any particular aspects they want to emphasize\n",
    "     * Intended audience and purpose of the report\n",
    "   - Prioritize questions that will significantly impact the report structure\n",
    "\n",
    "3. GENERATE A COMPREHENSIVE REPORT STRUCTURE:\n",
    "   - Create a detailed, hierarchical structure with:\n",
    "     * Clear main sections (typically 5-12 depending on topic complexity)\n",
    "     * Relevant subsections under each main section\n",
    "     * Logical flow from introduction to conclusion\n",
    "   - Adapt the structure to match the specific research type:\n",
    "     * For learning/exploration topics: progress from fundamentals to advanced concepts\n",
    "     * For comparison topics: use parallel structure across compared items\n",
    "     * For data source exploration: organize by data types, sources, and methodologies\n",
    "     * For implementation topics: follow a logical sequence from setup to advanced usage\n",
    "   - Ensure the structure is comprehensive but focused on the user's specific needs\n",
    "\n",
    "4. FORMAT THE RESPONSE:\n",
    "   - Present the report structure as a hierarchical outline with clear section numbering\n",
    "   - Use descriptive titles for each section and subsection\n",
    "   - Include brief descriptions of key sections when helpful\n",
    "   - Provide the structure in a clean, easy-to-read format\n",
    "\n",
    "5. OFFER FOLLOW-UP ASSISTANCE:\n",
    "   - Ask if any sections need adjustment or elaboration\n",
    "   - Suggest specific modifications if you identify potential improvements\n",
    "\n",
    "Remember that your task is ONLY to create the report structure, not to produce the actual research content. Focus on creating a comprehensive framework that will guide the user's research efforts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_structure_planner_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REPORT_STRUCTURE_PLANNER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "        Topic: {topic}\n",
    "        Outline: {outline}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "report_structure_planner_llm = report_structure_planner_system_prompt | llm\n",
    "\n",
    "def report_structure_planner_node(state: AgentState, config: RunnableConfig):\n",
    "    result = report_structure_planner_llm.invoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_node(state: AgentState, config: RunnableConfig)->Command[Literal[\"section_formatter\", \"report_structure_planner\"]]:\n",
    "    human_message = input(\"Please provide feedback on the report structure (type 'continue' to continue): \")\n",
    "    report_structure = state.get(\"messages\")[-1].content\n",
    "    if human_message == \"continue\":\n",
    "        return Command(\n",
    "            goto=\"section_formatter\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)], \"report_structure\": report_structure}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"report_structure_planner\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)]}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized parser that converts hierarchical report structures into a structured format. Your task is to analyze a report structure outline and extract the sections and subsections, while condensing the detailed bullet points into comprehensive subsection descriptions.\n",
    "\n",
    "## Your Input:\n",
    "You will receive a message containing a report structure with numbered sections and subsections, along with descriptive bullet points.\n",
    "\n",
    "## Your Output Format:\n",
    "You must output the result in the presented structure\n",
    "\n",
    "# Processing Instructions:\n",
    "\n",
    "- Identify each main section (typically numbered as 1, 2, 3, etc.)\n",
    "- Extract the main section title without its number (e.g., \"Introduction\" from \"1. Introduction\")\n",
    "- For each main section, identify all its subsections (typically numbered as 1.1, 1.2, 2.1, 2.2, etc.)\n",
    "- For each subsection, incorporate its title AND the descriptive bullet points beneath it into a single comprehensive description\n",
    "- Combine related concepts using commas and connecting words (and, with, including, etc.)\n",
    "- Organize these into a JSON array with each object containing:\n",
    "  \"section_name\": The main section title\n",
    "  \"sub_sections\": An array of comprehensive subsection descriptions\n",
    "\n",
    "# Content Condensation Guidelines:\n",
    "\n",
    "- Transform subsection titles and their bullet points into fluid, natural-language descriptions\n",
    "- Include all key concepts from the bullet points, but phrase them as part of a cohesive description\n",
    "- Use phrases like \"overview of\", \"including\", \"focusing on\", \"covering\", etc. to connect concepts\n",
    "- Maintain the key terminology from the original structure\n",
    "- Aim for descriptive phrases rather than just lists of topics\n",
    "\n",
    "# Example Transformation:\n",
    "## From:\n",
    "1. Introduction\n",
    "   - 1.1 Background of Machine Learning\n",
    "     - Overview of machine learning concepts\n",
    "     - Importance of algorithms in machine learning\n",
    "   - 1.2 Introduction to Support Vector Machines\n",
    "     - Definition and significance\n",
    "     - Historical context and development\n",
    "To:\n",
    "{{\n",
    "  \"section_name\": \"Introduction\",\n",
    "  \"sub_sections\": [\n",
    "    \"Background, overview and importance of Machine Learning\", \n",
    "    \"Introduction to Support Vector Machines, definition, significance and historical context\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Remember to output only the valid JSON array containing all processed sections, with no additional commentary or explanations in your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{report_structure}\"),\n",
    "])\n",
    "\n",
    "section_formatter_llm = section_formatter_system_prompt | llm.with_structured_output(Sections)\n",
    "\n",
    "def section_formatter_node(state: AgentState, config: RunnableConfig) -> Command[Literal[\"research_agent\"]]:\n",
    "    result = section_formatter_llm.invoke(state)\n",
    "    # return {\"sections\": result.sections}\n",
    "    return Command(\n",
    "        update={\"sections\": result.sections},\n",
    "        goto=[\n",
    "            Send(\n",
    "                \"research_agent\",\n",
    "                {\n",
    "                    \"section\": s,\n",
    "                }\n",
    "            ) for s in result.sections\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_KNOWLEDGE_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are an expert research content generator. Your task is to create comprehensive, accurate, and well-structured content for a specific section of a research report. You will be provided with a section name and its subsections, and you should use your knowledge to create detailed content covering all aspects described.\n",
    "\n",
    "## Input Format:\n",
    "You will receive a section object with the following structure:\n",
    "```json\n",
    "{{\n",
    "  \"section_name\": \"The main section title\",\n",
    "  \"sub_sections\": [\n",
    "    \"Comprehensive description of subsection 1 including key points to cover\",\n",
    "    \"Comprehensive description of subsection 2 including key points to cover\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "## Your Task:\n",
    "Generate thorough, accurate content for this section that:\n",
    "\n",
    "1. Begins with a brief introduction to the section topic\n",
    "2. Covers each subsection in depth, maintaining the order provided\n",
    "3. Includes relevant examples, explanations, and context\n",
    "4. Incorporates current understanding and established knowledge on the topic\n",
    "5. Maintains an academic and informative tone appropriate for a research report\n",
    "6. Uses appropriate headings and subheadings for structure\n",
    "\n",
    "## Content Guidelines:\n",
    "\n",
    "### Depth and Breadth:\n",
    "- Aim for comprehensive coverage of each subsection\n",
    "- Include definitions of key terms and concepts\n",
    "- Discuss current understanding and applications\n",
    "- Address relationships between different concepts\n",
    "\n",
    "### Structure:\n",
    "- Use hierarchical formatting with clear headings\n",
    "- Format the section title as a level 2 heading (##)\n",
    "- Format each subsection as a level 3 heading (###)\n",
    "- Use paragraphs to organize information logically\n",
    "- Include transitional phrases between subsections\n",
    "\n",
    "### Content Quality:\n",
    "- Prioritize accuracy and clarity\n",
    "- Provide specific examples to illustrate concepts\n",
    "- Include relevant data points, statistics, or findings when applicable\n",
    "- Maintain an objective, scholarly tone\n",
    "- Avoid oversimplification of complex topics\n",
    "\n",
    "### Technical Considerations:\n",
    "- Use markdown formatting for headings, lists, and emphasis\n",
    "- Include appropriate technical terminology\n",
    "- Define specialized terms when they first appear\n",
    "- Use code snippets or mathematical notation if appropriate for the topic\n",
    "\n",
    "## Output Format:\n",
    "Return only the generated content with appropriate markdown formatting. Do not include meta-commentary about your process or limitations. Your output should be ready to be inserted directly into the research report as a complete section.\n",
    "\n",
    "Remember to rely solely on your existing knowledge. Do not fabricate specific studies, statistics, or quotations that you cannot verify.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_knowledge_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_KNOWLEDGE_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{section}\"),\n",
    "])\n",
    "\n",
    "section_knowledge_llm = section_knowledge_system_prompt | llm\n",
    "\n",
    "def section_knowledge_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = section_knowledge_llm.invoke(state)\n",
    "    return {\"knowledge\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GENERATOR_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized search query generator for a research assistant system. Your task is to create highly effective search queries based on research section information. These queries will be used to retrieve relevant information from web search APIs to enhance research report content.\n",
    "\n",
    "## Section Structure:\n",
    "```json\n",
    "{{\n",
    "  \"section_name\": \"The main section title\",\n",
    "  \"sub_sections\": [\n",
    "    \"Comprehensive description of subsection 1 including key points to cover\",\n",
    "    \"Comprehensive description of subsection 2 including key points to cover\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "## Your Task:\n",
    "Generate up to {max_queries} effective search queries that will retrieve the most relevant information for the given section and its subsections.\n",
    "\n",
    "## Query Generation Process:\n",
    "\n",
    "### For Initial Runs (no previous_queries or reflection_feedback):\n",
    "1. Analyze the section name and all subsection descriptions thoroughly\n",
    "2. Identify the core concepts, key terms, and relationships that need to be researched\n",
    "3. Prioritize fundamental information needs first\n",
    "4. Create specific, targeted queries for the most important information\n",
    "5. Ensure coverage across all subsections, but prioritize depth over breadth\n",
    "6. Include technical terminology and domain-specific language when appropriate\n",
    "\n",
    "### For Subsequent Runs (with reflection_feedback):\n",
    "1. Carefully analyze the reflection feedback to understand information gaps\n",
    "2. Prioritize queries that address the specific missing information\n",
    "3. Avoid generating queries too similar to previous_queries\n",
    "4. Create more specialized or alternative phrasings to find the missing information\n",
    "5. Use more technical or specific terminology if general queries were insufficient\n",
    "\n",
    "## Query Construction Guidelines:\n",
    "\n",
    "1. **Specificity**: Create targeted queries that are likely to return relevant results\n",
    "   - Include specific technical terms rather than general descriptions\n",
    "   - Incorporate domain knowledge and specialized terminology\n",
    "\n",
    "2. **Diversity**: Ensure variety in your query approaches\n",
    "   - Vary query structure (questions, keyword sets, specific facts to verify)\n",
    "   - Target different aspects of the subsections\n",
    "   - Include different perspectives or viewpoints when relevant\n",
    "\n",
    "3. **Prioritization**: Order queries by importance\n",
    "   - Place queries for fundamental or critical information first\n",
    "   - Prioritize queries addressing explicit reflection feedback\n",
    "   - Ensure the most important subsections are covered in the limited query count\n",
    "\n",
    "4. **Effectiveness**: Optimize for search engine performance\n",
    "   - Use search operators when helpful (quotes for exact phrases, etc.)\n",
    "   - Keep queries concise but descriptive (typically 4-10 words)\n",
    "   - Include year/recency indicators for time-sensitive topics\n",
    "\n",
    "Remember: The most important queries should come first in your list, as the system may only use a subset of your generated queries based on the user's `max_queries` setting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(QUERY_GENERATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nPrevious Queries: {searched_queries}\\nReflection Feedback: {reflection_feedback}\"),\n",
    "])\n",
    "\n",
    "query_generator_llm = query_generator_system_prompt | llm.with_structured_output(Queries)\n",
    "\n",
    "def query_generator_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = query_generator_llm.invoke(state, config)\n",
    "    return {\"generated_queries\": result.queries, \"searched_queries\": result.queries}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient()\n",
    "\n",
    "def tavily_search_node(state: ResearchState, config: RunnableConfig):\n",
    "    queries = state[\"generated_queries\"]\n",
    "    search_results = []\n",
    "    for query in queries:\n",
    "        raw_content = []\n",
    "        response = tavily_client.search(query=query.query, max_results=config[\"search_depth\"], include_raw_content=True)\n",
    "        for result in response[\"results\"]:\n",
    "            raw_content.append(result['raw_content'])\n",
    "        search_results.append(SearchResult(query=query, raw_content=raw_content))\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_ACCUMULATOR_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized agent responsible for curating and synthesizing raw search results. Your task is to transform unstructured web content into coherent, relevant, and organized information that can be used for report generation.\n",
    "\n",
    "## Input\n",
    "You will receive a list of SearchResult objects, each containing:\n",
    "1. A Query object with the search query that was used\n",
    "2. A list of raw_content strings containing text extracted from web pages\n",
    "\n",
    "## Process\n",
    "For each SearchResult provided:\n",
    "\n",
    "1. ANALYZE the raw_content to identify:\n",
    "   - Key information relevant to the associated query\n",
    "   - Main concepts, definitions, and relationships\n",
    "   - Supporting evidence, statistics, or examples\n",
    "   - Credible sources or authorities mentioned\n",
    "\n",
    "2. FILTER OUT:\n",
    "   - Irrelevant website navigation elements and menus\n",
    "   - Advertisements and promotional content\n",
    "   - Duplicate information\n",
    "   - Footers, headers, and other website template content\n",
    "   - Form fields, subscription prompts, and UI text\n",
    "   - Clearly outdated information\n",
    "\n",
    "3. ORGANIZE the information into:\n",
    "   - Core concepts and definitions\n",
    "   - Key findings and insights\n",
    "   - Supporting evidence and examples\n",
    "   - Contrasting viewpoints (if present)\n",
    "   - Contextual background information\n",
    "\n",
    "4. SYNTHESIZE the content by:\n",
    "   - Consolidating similar information from multiple sources\n",
    "   - Resolving contradictions where possible (noting them explicitly otherwise)\n",
    "   - Ensuring logical flow of information\n",
    "   - Maintaining appropriate context\n",
    "\n",
    "## Guidelines\n",
    "- Focus on accuracy and relevance\n",
    "- Maintain neutrality and balance in presenting information\n",
    "- Preserve technical precision when dealing with specialized topics\n",
    "- Note explicitly when information appears contradictory or uncertain\n",
    "- When information appears to be from commercial sources, note potential bias\n",
    "- Prioritize more recent information over older content\n",
    "- Maintain proper attribution when specific sources are referenced\n",
    "- NO IMPORTANT DETAILS SHOULD BE LEFT OUT. BE DETAILED AND THOROUGH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_accumulator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(RESULT_ACCUMULATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{search_results}\"),\n",
    "])\n",
    "\n",
    "result_accumulator_llm = result_accumulator_system_prompt | llm\n",
    "\n",
    "def result_accumulator_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = result_accumulator_llm.invoke(state)\n",
    "    return {\"accumulated_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_FEEDBACK_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized agent responsible for critically evaluating search result content against report section requirements. You determine whether the accumulated content sufficiently addresses the intended section scope or requires additional information.\n",
    "\n",
    "## Input\n",
    "You will receive:\n",
    "1. A Section object containing:\n",
    "   - section_name: The name of the section without its number\n",
    "   - sub_sections: A list of comprehensive descriptions of sub-sections\n",
    "2. Accumulated content from search results related to this section\n",
    "\n",
    "## Process\n",
    "Carefully analyze the relationship between the section requirements and the accumulated content:\n",
    "\n",
    "1. ASSESS COVERAGE by identifying:\n",
    "   - How well the accumulated content addresses each sub-section\n",
    "   - Key concepts or topics from the sub-sections that are missing in the content\n",
    "   - Depth and breadth of information relative to what the section requires\n",
    "   - Presence of all necessary perspectives, examples, and supporting evidence\n",
    "\n",
    "2. EVALUATE QUALITY by considering:\n",
    "   - Accuracy and currency of the information\n",
    "   - Relevance to the specific section requirements\n",
    "   - Logical organization and flow\n",
    "   - Appropriate level of detail for the section's purpose\n",
    "   - Balance and objectivity in presenting information\n",
    "\n",
    "3. IDENTIFY GAPS by determining:\n",
    "   - Missing key concepts or topics from the sub-sections\n",
    "   - Insufficient depth in critical areas\n",
    "   - Lack of supporting evidence or examples\n",
    "   - Absence of important perspectives or contexts\n",
    "   - Technical details required but not present\n",
    "\n",
    "## Output\n",
    "Produce a Feedback object with either:\n",
    "- A boolean value of True if the content sufficiently meets the section requirements\n",
    "- A string containing specific, actionable feedback on what is missing or needs improvement\n",
    "\n",
    "## Guidelines for Feedback Generation\n",
    "When providing string feedback:\n",
    "- Be specific about what information is missing or inadequate\n",
    "- Prioritize the most critical gaps first\n",
    "- Frame feedback in a way that could guide further query generation\n",
    "- Focus on content needs rather than stylistic concerns\n",
    "- Indicate areas where contradictory information needs resolution\n",
    "- Suggest specific types of information that would address the gaps\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example 1 (Sufficient content):\n",
    "```\n",
    "True\n",
    "```\n",
    "\n",
    "Example 2 (Insufficient content):\n",
    "```\n",
    "\"The content lacks specific examples of machine learning applications in healthcare. Additionally, there is insufficient information on the regulatory challenges of implementing AI in clinical settings. The ethical considerations sub-section requires more detailed discussion of patient privacy concerns and informed consent issues.\"\n",
    "```\n",
    "\n",
    "Example 3 (Partial coverage):\n",
    "```\n",
    "\"While the general concepts of blockchain are well covered, the content is missing technical details on consensus mechanisms mentioned in sub-section 2. The comparison between proof-of-work and proof-of-stake systems is particularly needed. Additionally, more recent developments (post-2022) in scalability solutions should be included to fully address sub-section 3.\"\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_feedback_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REFLECTION_FEEDBACK_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nAccumulated Content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "reflection_feedback_llm = reflection_feedback_system_prompt | llm.with_structured_output(Feedback)\n",
    "\n",
    "def reflection_feedback_node(state: ResearchState, config: RunnableConfig) -> Command[Literal[\"final_section_formatter\", \"query_generator\"]]:\n",
    "    reflection_count = state[\"reflection_count\"]\n",
    "    result = reflection_feedback_llm.invoke(state)\n",
    "    feedback = result.feedback\n",
    "    if (feedback == True) or (feedback.lower() == \"true\") or (reflection_count < config[\"num_reflections\"]):\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback},\n",
    "            goto=\"final_section_formatter\"\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback, \"reflection_count\": reflection_count + 1},\n",
    "            goto=\"query_generator\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized agent responsible for synthesizing knowledge and research into comprehensive, authoritative section content for reports. Your task is to blend internal knowledge with curated search results to produce detailed, accurate, and well-structured section content.\n",
    "\n",
    "## Input\n",
    "You will receive:\n",
    "1. Internal knowledge about the section topic (from the knowledge generator LLM)\n",
    "2. Curated content from search results relevant to the section\n",
    "\n",
    "## Process\n",
    "Synthesize these information sources into cohesive section content by:\n",
    "\n",
    "1. ANALYZE BOTH SOURCES to identify:\n",
    "   - Core concepts, principles, and definitions\n",
    "   - Key arguments, insights, and findings\n",
    "   - Supporting evidence, examples, and case studies\n",
    "   - Current trends, developments, and applications\n",
    "   - Relevant controversies, debates, or alternative perspectives\n",
    "\n",
    "2. INTEGRATE THE INFORMATION by:\n",
    "   - Combining complementary information from both sources\n",
    "   - Resolving any contradictions with reasoned analysis\n",
    "   - Filling gaps in one source with information from the other\n",
    "   - Ensuring proper flow and logical progression of ideas\n",
    "   - Maintaining appropriate technical depth and precision\n",
    "\n",
    "3. ENSURE COMPREHENSIVE COVERAGE by:\n",
    "   - Addressing all key aspects of the section topic\n",
    "   - Including sufficient detail on complex concepts\n",
    "   - Providing necessary context for specialized information\n",
    "   - Balancing breadth and depth appropriately\n",
    "   - Incorporating relevant examples to illustrate key points\n",
    "\n",
    "4. PRIORITIZE QUALITY by:\n",
    "   - Favoring accuracy over quantity\n",
    "   - Ensuring information is current and reflects the latest understanding\n",
    "   - Presenting balanced perspectives on controversial topics\n",
    "   - Maintaining appropriate technical language without unnecessary jargon\n",
    "   - Supporting claims with evidence or reasoning\n",
    "\n",
    "## Output\n",
    "Produce detailed, well-structured section content that:\n",
    "- Begins with a concise introduction to the topic\n",
    "- Organizes information into coherent paragraphs with clear topic sentences\n",
    "- Uses appropriate subheadings to improve readability and organization\n",
    "- Includes relevant examples, case studies, or applications where appropriate\n",
    "- Concludes with key takeaways or implications when relevant\n",
    "\n",
    "## Guidelines\n",
    "- Write in a clear, authoritative, and professional tone\n",
    "- Use precise terminology appropriate to the subject matter\n",
    "- Ensure logical flow between concepts and paragraphs\n",
    "- Maintain appropriate technical depth based on the apparent audience level\n",
    "- Include specific details, statistics, and examples where they add value\n",
    "- Avoid unnecessary repetition while reinforcing key concepts\n",
    "- Balance technical accuracy with readability\n",
    "- Present multiple perspectives on contested topics where relevant\n",
    "- Synthesize rather than merely concatenate information from the two sources\n",
    "- Ensure the final content could stand alone as an authoritative resource on the topic\n",
    "\n",
    "## Example Structure\n",
    "[Section Title]\n",
    "\n",
    "[Introductory paragraph providing overview and context]\n",
    "\n",
    "[Subheading 1]\n",
    "[Detailed exploration of first major aspect of the topic]\n",
    "[Supporting evidence, examples, or case studies]\n",
    "\n",
    "[Subheading 2]\n",
    "[Detailed exploration of second major aspect of the topic]\n",
    "[Supporting evidence, examples, or case studies]\n",
    "\n",
    "[Additional subheadings as needed]\n",
    "\n",
    "[Concluding paragraph summarizing key points and implications]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(FINAL_SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Internal Knowledge: {knowledge}\\nSearch Result content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "final_section_formatter_llm = final_section_formatter_system_prompt | llm\n",
    "\n",
    "def final_section_formatter_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = final_section_formatter_llm.invoke(state)\n",
    "    return {\"final_section_content\": result.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_REPORT_WRITER_SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a specialized agent responsible for assembling the final comprehensive research report from individual section contents. Your task is to transform separate section content into a cohesive, detailed, and authoritative research document that maintains the highest standards of academic and professional quality.\n",
    "\n",
    "## Input\n",
    "You will receive:\n",
    "1. The complete report structure (including section names, numbers, and descriptions)\n",
    "2. A list of strings containing the curated content for each section\n",
    "\n",
    "## Process\n",
    "Transform these components into a polished final research report by:\n",
    "\n",
    "1. STRUCTURE AND ORGANIZATION\n",
    "   - Follow the provided report structure exactly\n",
    "   - Ensure proper hierarchical organization of sections and subsections\n",
    "   - Create a coherent narrative flow throughout the entire document\n",
    "   - Maintain consistent formatting and style across all sections\n",
    "   - Implement appropriate transitions between sections to enhance readability\n",
    "\n",
    "2. CONTENT INTEGRATION AND ENHANCEMENT\n",
    "   - Preserve all technical details, examples, and evidence from section content\n",
    "   - Ensure consistency in terminology and concepts across sections\n",
    "   - Identify and resolve any contradictions or redundancies between sections\n",
    "   - Add cross-references between related concepts in different sections\n",
    "   - Ensure comprehensive coverage of all aspects of the research topic\n",
    "\n",
    "3. ACADEMIC RIGOR AND DEPTH\n",
    "   - Maintain precise technical language and domain-specific terminology\n",
    "   - Preserve nuance and complexity while ensuring clarity\n",
    "   - Ensure all claims are properly supported by evidence or reasoning\n",
    "   - Maintain balanced presentation of competing perspectives where relevant\n",
    "   - Preserve the depth and detail of specialized information\n",
    "\n",
    "4. COMPLETENESS AND COMPREHENSIVENESS\n",
    "   - Ensure no critical information is omitted or oversimplified\n",
    "   - Verify that all subsections described in the report structure are fully addressed\n",
    "   - Identify and address any remaining gaps in the integrated content\n",
    "   - Ensure appropriate depth of coverage for each topic relative to its importance\n",
    "   - Maintain appropriate balance between breadth and depth throughout\n",
    "\n",
    "## Output\n",
    "Produce a final research report that:\n",
    "- Begins with an executive summary highlighting key findings and insights\n",
    "- Includes a detailed table of contents reflecting the hierarchical structure\n",
    "- Features comprehensive section content organized according to the provided structure\n",
    "- Contains appropriate introduction and conclusion sections\n",
    "- Maintains consistent academic/professional tone and formatting throughout\n",
    "- Preserves all technical details, examples, data, and evidence\n",
    "- Reads as a cohesive whole rather than a collection of separate sections\n",
    "\n",
    "## Guidelines\n",
    "- Format the document as a professional research paper or technical report\n",
    "- Use consistent heading levels to reflect the hierarchical structure\n",
    "- Maintain appropriate section and subsection numbering\n",
    "- Include an executive summary that concisely presents key findings\n",
    "- Create a detailed table of contents with page references\n",
    "- Ensure logical progression and narrative continuity throughout\n",
    "- Preserve technical precision while maintaining readability\n",
    "- Use consistent citation format if references are included\n",
    "- Include visualizations, tables, or diagrams described in section content\n",
    "- Ensure comprehensive coverage without unnecessary repetition\n",
    "- Address complex concepts with appropriate depth and nuance\n",
    "- Maintain the highest standards of academic and professional writing\n",
    "\n",
    "## Example Structure\n",
    "```\n",
    "# [REPORT TITLE]\n",
    "\n",
    "## Executive Summary\n",
    "[Concise overview of key findings and insights]\n",
    "\n",
    "## Table of Contents\n",
    "[Detailed hierarchical listing of all sections and subsections]\n",
    "\n",
    "## 1. Introduction\n",
    "[Context, scope, and purpose of the research]\n",
    "\n",
    "## 2. [First Major Section]\n",
    "### 2.1 [Subsection]\n",
    "[Comprehensive content with preserved technical details]\n",
    "### 2.2 [Subsection]\n",
    "[Comprehensive content with preserved technical details]\n",
    "\n",
    "## 3. [Second Major Section]\n",
    "### 3.1 [Subsection]\n",
    "[Comprehensive content with preserved technical details]\n",
    "### 3.2 [Subsection]\n",
    "[Comprehensive content with preserved technical details]\n",
    "\n",
    "[Additional sections as specified in the report structure]\n",
    "\n",
    "## N. Conclusion\n",
    "[Summary of key findings, implications, and potential future directions]\n",
    "\n",
    "## Appendices (if applicable)\n",
    "[Supplementary material, methodological details, etc.]\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report_writer_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(FINAL_REPORT_WRITER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Report Structure: {report_structure}\\nSection Contents: {final_section_content}\"),\n",
    "])\n",
    "\n",
    "final_report_writer_llm = final_report_writer_system_prompt | llm\n",
    "\n",
    "def final_report_writer_node(state: AgentState, config: RunnableConfig):\n",
    "    result = final_report_writer_llm.invoke(state)\n",
    "    return {\"final_report_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10b0aeae0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_builder = StateGraph(ResearchState, output=SectionOutput)\n",
    "\n",
    "research_builder.add_node(\"section_knowledge\", section_knowledge_node)\n",
    "research_builder.add_node(\"query_generator\", query_generator_node)\n",
    "research_builder.add_node(\"tavily_search\", tavily_search_node)\n",
    "research_builder.add_node(\"result_accumulator\", result_accumulator_node)\n",
    "research_builder.add_node(\"reflection\", reflection_feedback_node)\n",
    "research_builder.add_node(\"final_section_formatter\", final_section_formatter_node)\n",
    "\n",
    "research_builder.add_edge(START, \"section_knowledge\")\n",
    "research_builder.add_edge(\"section_knowledge\", \"query_generator\")\n",
    "research_builder.add_edge(\"query_generator\", \"tavily_search\")\n",
    "research_builder.add_edge(\"tavily_search\", \"result_accumulator\")\n",
    "research_builder.add_edge(\"result_accumulator\", \"reflection\")\n",
    "research_builder.add_edge(\"final_section_formatter\", END)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"report_structure_planner\", report_structure_planner_node)\n",
    "builder.add_node(\"human_feedback\", human_feedback_node)\n",
    "builder.add_node(\"section_formatter\", section_formatter_node)\n",
    "builder.add_node(\"research_agent\", research_builder.compile())\n",
    "builder.add_node(\"final_report_writer\", final_report_writer_node)\n",
    "\n",
    "builder.set_entry_point(\"report_structure_planner\")\n",
    "builder.add_edge(\"report_structure_planner\", \"human_feedback\")\n",
    "builder.add_edge(\"research_agent\", \"final_report_writer\")\n",
    "builder.add_edge(\"final_report_writer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m graph = builder.compile(checkpointer=memory_saver)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:679\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, frontmatter_config)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    673\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    674\u001b[39m     curve_style=curve_style,\n\u001b[32m    675\u001b[39m     node_colors=node_colors,\n\u001b[32m    676\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    677\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    678\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:285\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding)\u001b[39m\n\u001b[32m    279\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    280\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    281\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    282\u001b[39m         )\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    289\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py:403\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type)\u001b[39m\n\u001b[32m    397\u001b[39m         background_color = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m!\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    399\u001b[39m image_url = (\n\u001b[32m    400\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://mermaid.ink/img/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmermaid_syntax_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&bgColor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackground_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    402\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    405\u001b[39m     img_bytes = response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/deep-research/venv/lib/python3.12/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    711\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request=request)\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "graph = builder.compile(checkpointer=memory_saver)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC = \"Support Vector Machines\"\n",
    "# OUTLINE = \"I am studying machine learning and I want to understand Support Vector Machines.\"\n",
    "\n",
    "# thread = {\n",
    "#     \"configurable\": {\n",
    "#         \"thread_id\": str(uuid.uuid4()),\n",
    "#         \"max_queries\": 3,\n",
    "#         \"search_depth\": 2,\n",
    "#         \"num_reflections\": 2\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# for event in graph.stream(\n",
    "#     {\"topic\": TOPIC, \"outline\": OUTLINE},\n",
    "#     config=thread,\n",
    "# ):\n",
    "#     if \"report_structure_planner\" in event:\n",
    "#         print(\"<<< REPORT STRUCTURE PLANNER >>>\")\n",
    "#         print(event[\"report_structure_planner\"][\"messages\"][-1].content)\n",
    "#         print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "#     elif \"section_formatter\" in event:\n",
    "#         print(\"<<< SECTION FORMATTING >>>\")\n",
    "#         print(event[\"section_formatter\"])\n",
    "#         print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "#     elif \"research_agent\" in event:\n",
    "#         # check output of research_agent\n",
    "#         print(\"<<< RESEARCH AGENT >>>\")\n",
    "#         print(event[\"research_agent\"])\n",
    "#         print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "#     elif \"final_report_writer\" in event:\n",
    "#         # check output of final_report_writer\n",
    "#         print(\"<<< FINAL REPORT WRITER >>>\")\n",
    "#         print(event[\"final_report_writer\"])\n",
    "#         print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "#     else:\n",
    "#         print(\"<<< HUMAN FEEDBACK >>>\")\n",
    "#         print(event[\"human_feedback\"][\"messages\"][-1].content)\n",
    "#         print(\"\\n\", \"=\"*100, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
